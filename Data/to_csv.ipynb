{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2018/201800002') as fd:\n",
    "    data = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The measurement of drug-induced interferon γ-releasing cells and lymphocyte proliferation in severe cutaneous adverse reactions\n"
     ]
    }
   ],
   "source": [
    "# tiltle\n",
    "title = data['abstracts-retrieval-response']['item']['bibrecord']['head']['citation-title']\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/06/2018\n"
     ]
    }
   ],
   "source": [
    "# pub_date \n",
    "pub_year = data['abstracts-retrieval-response']['item']['bibrecord']['head']['source']['publicationdate']['year']\n",
    "pub_month = data['abstracts-retrieval-response']['item']['bibrecord']['head']['source']['publicationdate']['month']\n",
    "pub_day = data['abstracts-retrieval-response']['item']['bibrecord']['head']['source']['publicationdate']['day']\n",
    "pub_date = pub_day + '/' + pub_month + '/' + pub_year\n",
    "print(pub_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "© 2018 European Academy of Dermatology and VenereologyBackground: The lymphocyte transformation test (LTT) is a standard laboratory method to identify culprit drugs in patients with a history of drug-induced non-immediate hypersensitivity and is mainly performed during the recovery phase. The measurement of drug-specific interferon γ (IFN-γ)-releasing cells has been introduced to confirm culprit drugs, even during the acute phase of drug allergy. Objectives: This study aimed to evaluate the capability of the enzyme-linked immunospot assay (ELISpot) to detect drug-specific IFN-γ-releasing cells during the acute phase and the capability of LTT to identify culprit drugs during the recovery phase in patients presenting with severe cutaneous adverse reactions (SCARs). Methods: Peripheral blood mononuclear cells (PBMCs) from 23 SCAR patients were collected during the acute and recovery phases and assayed for drug-specific IFN-γ-releasing cells and lymphocyte proliferation, respectively. Results: Drug-specific IFN-γ-releasing cells were detectable in 73.9% of SCAR subjects (55.6% and 85.7% in patients who were and were not taking systemic steroids, respectively), whereas LTT results were positive in 52.2% of SCAR subjects. The frequencies of drug-specific IFN-γ-releasing cells were significantly higher in patients with positive LTT than in those with negative LTT (260.1 ± 110.0 and 46.6 ± 20.7 cells/106 PBMCs, P = 0.01). A significant correlation between the results of the IFN-γ ELISpot assay and LTT was demonstrated (r = 0.65, P value <0.01). Conclusion: The IFN-γ ELISpot assay could be a useful tool to identify culprit drugs in SCAR patients when culprit drug identification is urgently needed during the acute phase of drug allergy.\n"
     ]
    }
   ],
   "source": [
    "# abstract \n",
    "abstract = data['abstracts-retrieval-response']['item']['bibrecord']['head']['abstracts']\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n"
     ]
    }
   ],
   "source": [
    "# language \n",
    "language = data['abstracts-retrieval-response']['item']['bibrecord']['head']['citation-info']['citation-language']['@language']\n",
    "print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# refcount  \n",
    "ref_count = data['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']['@refcount']\n",
    "print(ref_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# citedcount \n",
    "citedby_count = data['abstracts-retrieval-response']['coredata']['citedby-count']\n",
    "print(citedby_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['57189870740', '6504156938', '26532099300', '16403174400', '7003678340', '26024040600', '44561414300', '41261158800', '37017011900', '57190868628', '56871252100', '55418940200', '57209265337', '6508090526']\n"
     ]
    }
   ],
   "source": [
    "# author_id \n",
    "authors_list = data['abstracts-retrieval-response']['authors']['author']\n",
    "author_id = []\n",
    "for author in authors_list:\n",
    "    author_id.append(author['@auid'])\n",
    "print(author_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'$'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classification_list:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 6\u001b[0m         classification_code\u001b[38;5;241m.\u001b[39mappend(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m$\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# classification_code.append(c['classification'])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_code)\n",
      "\u001b[1;31mKeyError\u001b[0m: '$'"
     ]
    }
   ],
   "source": [
    "# classification_code\n",
    "classification_list = data['abstracts-retrieval-response']['item']['bibrecord']['head']['enhancement']['classificationgroup']['classifications']  \n",
    "classification_code = []\n",
    "for c in classification_list:\n",
    "    for x in c['classification']:\n",
    "        classification_code.append(x['$'])\n",
    "    # classification_code.append(c['classification'])\n",
    "print(classification_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Results saved to paper_2019.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def safe_get(data, path, default=None):\n",
    "    \"\"\"\n",
    "    Safely retrieves a value from a nested dictionary or list.\n",
    "    \n",
    "    :param data: The dictionary or list to retrieve data from.\n",
    "    :param path: A list of keys/indexes representing the path to the desired value.\n",
    "    :param default: The value to return if the path does not exist or is invalid.\n",
    "    :return: The retrieved value or the default value.\n",
    "    \"\"\"\n",
    "    for key in path:\n",
    "        if isinstance(data, dict):\n",
    "            data = data.get(key, default)\n",
    "        elif isinstance(data, list) and isinstance(key, int) and 0 <= key < len(data):\n",
    "            data = data[key]\n",
    "        else:\n",
    "            return default\n",
    "    return data\n",
    "\n",
    "def check_slice_or_single(data):\n",
    "    \"\"\"\n",
    "    Ensures that the returned data is always a list, even if it's a single item.\n",
    "    \n",
    "    :param data: The data to check, which can be a list or a single value.\n",
    "    :return: A list of data values.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    return [data]\n",
    "\n",
    "def get_classification_codes(classification_list):\n",
    "    \"\"\"\n",
    "    Processes the classification list and extracts the relevant classification code.\n",
    "    \n",
    "    :param classification_list: List of classifications, can be of different types.\n",
    "    :return: List of classification codes.\n",
    "    \"\"\"\n",
    "    classification_codes = []\n",
    "    \n",
    "    for classification in classification_list:\n",
    "        if isinstance(classification, dict):\n",
    "            classification_type = classification.get('@type')\n",
    "            classification_data = classification.get('classification')\n",
    "            \n",
    "            if classification_type == 'ASJC':\n",
    "                if isinstance(classification_data, list):\n",
    "                    # For ASJC, extract $ field from all items in the list\n",
    "                    classification_codes.extend([item.get('$') for item in classification_data if isinstance(item, dict)])\n",
    "                elif isinstance(classification_data, dict):\n",
    "                    # For ASJC, extract $ field from single item\n",
    "                    classification_codes.append(classification_data.get('$'))\n",
    "            \n",
    "            elif classification_type == 'SUBJABBR':\n",
    "                if isinstance(classification_data, list):\n",
    "                    # For SUBJABBR, extract $ field from all items in the list\n",
    "                    classification_codes.extend([item.get('$') for item in classification_data if isinstance(item, dict)])\n",
    "                elif isinstance(classification_data, dict):\n",
    "                    # For SUBJABBR, extract $ field from single item\n",
    "                    classification_codes.append(classification_data.get('$'))\n",
    "            \n",
    "            elif classification_type in ['CPXCLASS', 'FLXCLASS']:\n",
    "                if isinstance(classification_data, list):\n",
    "                    # For CPXCLASS and FLXCLASS, extract classification-code from all items in the list\n",
    "                    classification_codes.extend([item.get('classification-code') for item in classification_data if isinstance(item, dict)])\n",
    "                elif isinstance(classification_data, dict):\n",
    "                    # For CPXCLASS and FLXCLASS, extract classification-code from single item\n",
    "                    classification_codes.append(classification_data.get('classification-code'))\n",
    "    \n",
    "    return classification_codes\n",
    "\n",
    "folder_path = '2018'\n",
    "output_csv = 'paper_2019.csv'\n",
    "\n",
    "columns = [\n",
    "    'title', 'pub_date', 'abstract', 'language', 'ref_count',\n",
    "    'citedby_count', 'author_id', 'subject_areas_id', 'keywords',\n",
    "    'idxterms', 'classification_code'\n",
    "]\n",
    "\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as fd:\n",
    "                data = json.load(fd)\n",
    "\n",
    "                title = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'citation-title'], None)\n",
    "                \n",
    "                pub_year = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'year'], None)\n",
    "                pub_month = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'month'], None)\n",
    "                pub_day = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'day'], None)\n",
    "                pub_date = f\"{pub_day}/{pub_month}/{pub_year}\" if pub_year and pub_month and pub_day else None\n",
    "\n",
    "                abstract = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'abstracts'], None)\n",
    "\n",
    "                language = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'citation-info', 'citation-language', '@language'], None)\n",
    "\n",
    "                ref_count = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'tail', 'bibliography', '@refcount'], None)\n",
    "\n",
    "                citedby_count = safe_get(data, ['abstracts-retrieval-response', 'coredata', 'citedby-count'], None)\n",
    "\n",
    "                authors_list = safe_get(data, ['abstracts-retrieval-response', 'authors', 'author'], None)\n",
    "                author_id = [author.get('@auid', None) for author in authors_list if isinstance(author, dict)] if authors_list else None\n",
    "\n",
    "                subject_areas_list = safe_get(data, ['abstracts-retrieval-response', 'subject-areas', 'subject-area'], None)\n",
    "                subject_areas_id = [subject_area.get('@code', None) for subject_area in subject_areas_list if isinstance(subject_area, dict)] if subject_areas_list else None\n",
    "\n",
    "                authkeywords = safe_get(data, ['abstracts-retrieval-response', 'citation-info', 'author-keywords', 'author-keyword'], None)\n",
    "                # Check if authkeywords is not None and handle both single and multiple keywords\n",
    "                if authkeywords:\n",
    "                    if isinstance(authkeywords, dict):\n",
    "                        # Single author-keyword in File 1, wrap it in a list\n",
    "                        keywords = [authkeywords.get('$', None)]\n",
    "                    else:\n",
    "                        # Multiple author-keywords in File 2\n",
    "                        keywords = [k.get('$', None) for k in authkeywords]\n",
    "                else:\n",
    "                    keywords = None\n",
    "                keywords = check_slice_or_single(keywords)\n",
    "\n",
    "                idxterms_data = safe_get(data, ['abstracts-retrieval-response', 'idxterms'], None)\n",
    "                if isinstance(idxterms_data, dict):  # Handle case when idxterms is a dictionary\n",
    "                    mainterm = idxterms_data.get('mainterm')\n",
    "                    idxterms = [mainterm.get('$', None)] if isinstance(mainterm, dict) else None\n",
    "                elif isinstance(idxterms_data, list):  # Handle case when idxterms is a list\n",
    "                    idxterms = [i.get('$', None) for i in idxterms_data.get('mainterm', [])] if idxterms_data else None\n",
    "                else:\n",
    "                    idxterms = None\n",
    "\n",
    "                classification_list = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'enhancement', 'classificationgroup', 'classifications'], None)\n",
    "                classification_code = get_classification_codes(classification_list)\n",
    "                classification_code = check_slice_or_single(classification_code)\n",
    "\n",
    "                writer.writerow({\n",
    "                    'title': title,\n",
    "                    'pub_date': pub_date,\n",
    "                    'abstract': abstract,\n",
    "                    'language': language,\n",
    "                    'ref_count': ref_count,\n",
    "                    'citedby_count': citedby_count,\n",
    "                    'author_id': author_id,\n",
    "                    'subject_areas_id': subject_areas_id,\n",
    "                    'keywords': keywords,\n",
    "                    'idxterms': idxterms,\n",
    "                    'classification_code': classification_code\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "print(f\"Data extraction complete. Results saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2018/201800036') as fd:\n",
    "    data4 = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2708', '2725']\n"
     ]
    }
   ],
   "source": [
    "# subject_areas_id\n",
    "subject_areas_list = data['abstracts-retrieval-response']['subject-areas']['subject-area']\n",
    "subject_area_id = []\n",
    "for subject_area in subject_areas_list:\n",
    "    subject_area_id.append(subject_area['@code'])\n",
    "print(subject_area_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabstracts-retrieval-response\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthkeywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauthor-keyword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(keywords)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "keywords = data['abstracts-retrieval-response']['authkeywords']['author-keyword'][0]['$']\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Circulating fluidized bed', 'Computational fluid dynamics', 'Multiphase flow models', 'Riser', 'Sorption enhanced steam methane reforming']\n"
     ]
    }
   ],
   "source": [
    "# keywords\n",
    "keywords = None\n",
    "if data['abstracts-retrieval-response']['authkeywords'] == None:\n",
    "    keywords = None\n",
    "else:\n",
    "    keywords = []\n",
    "    for k in data['abstracts-retrieval-response']['authkeywords']['author-keyword']:\n",
    "        keywords.append(k['$'])\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2018/201800479') as fd:\n",
    "    data2 = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2018/201800017') as fd:\n",
    "    data3 = json.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite photocatalysts\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "a = data3['abstracts-retrieval-response']['idxterms']['mainterm'][0]['$']\n",
    "b = data['abstracts-retrieval-response']['idxterms']\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Circulating fluidized bed', 'Circulating fluidized bed riser', 'Full factorial design', 'Multi-phase flow models', 'Ni-based catalyst', 'Radial distributions', 'Reaction parameters', 'Riser']\n"
     ]
    }
   ],
   "source": [
    "# idxterms\n",
    "idxterms = None\n",
    "if data['abstracts-retrieval-response']['idxterms'] == None:\n",
    "    idxterms = None\n",
    "else:\n",
    "    idxterms = []\n",
    "    for i in data['abstracts-retrieval-response']['idxterms']['mainterm']:\n",
    "        idxterms.append(i['$'])\n",
    "print(idxterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Results saved to paper_2023.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def safe_get(data, path, default=None):\n",
    "    \"\"\"\n",
    "    Safely retrieves a value from a nested dictionary or list.\n",
    "    \n",
    "    :param data: The dictionary or list to retrieve data from.\n",
    "    :param path: A list of keys/indexes representing the path to the desired value.\n",
    "    :param default: The value to return if the path does not exist or is invalid.\n",
    "    :return: The retrieved value or the default value.\n",
    "    \"\"\"\n",
    "    for key in path:\n",
    "        if isinstance(data, dict):\n",
    "            data = data.get(key, default)\n",
    "        elif isinstance(data, list) and isinstance(key, int) and 0 <= key < len(data):\n",
    "            data = data[key]\n",
    "        else:\n",
    "            return default\n",
    "    return data\n",
    "\n",
    "def check_slice_or_single(data):\n",
    "    \"\"\"\n",
    "    Ensures that the returned data is always a list, even if it's a single item.\n",
    "    \n",
    "    :param data: The data to check, which can be a list or a single value.\n",
    "    :return: A list of data values.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    return [data]\n",
    "\n",
    "def get_classification_codes(classification_list):\n",
    "    \"\"\"\n",
    "    Processes the classification list and extracts the relevant classification code.\n",
    "    \n",
    "    :param classification_list: List of classifications, can be of different types.\n",
    "    :return: List of classification codes.\n",
    "    \"\"\"\n",
    "    classification_codes = []\n",
    "    \n",
    "    for classification in classification_list:\n",
    "        if isinstance(classification, dict):\n",
    "            classification_type = classification.get('@type')\n",
    "            classification_data = classification.get('classification')\n",
    "            \n",
    "            if classification_type == 'ASJC':\n",
    "                # For ASJC, classification might be a single string or a list\n",
    "                if isinstance(classification_data, list):\n",
    "                    classification_codes.extend([item.get('$') for item in classification_data if isinstance(item, dict)])\n",
    "                else:\n",
    "                    classification_codes.append(classification_data)  # Single string classification\n",
    "            \n",
    "            elif classification_type == 'SUBJABBR':\n",
    "                # For SUBJABBR, classification might be a single string or a list\n",
    "                if isinstance(classification_data, list):\n",
    "                    classification_codes.extend([item.get('$') for item in classification_data if isinstance(item, dict)])\n",
    "                else:\n",
    "                    classification_codes.append(classification_data)  # Single string classification\n",
    "            \n",
    "            elif classification_type in ['CPXCLASS', 'FLXCLASS']:\n",
    "                if isinstance(classification_data, list):\n",
    "                    classification_codes.extend([item.get('classification-code') for item in classification_data if isinstance(item, dict)])\n",
    "                elif isinstance(classification_data, dict):\n",
    "                    classification_codes.append(classification_data.get('classification-code'))\n",
    "    \n",
    "    return classification_codes\n",
    "\n",
    "\n",
    "folder_path = '2023'\n",
    "output_csv = 'paper_2023.csv'\n",
    "\n",
    "columns = [\n",
    "    'pid','title', 'pub_date', 'abstract', 'language', 'ref_count',\n",
    "    'citedby_count', 'author_id', 'subject_areas_id', 'keywords',\n",
    "    'idxterms', 'classification_code'\n",
    "]\n",
    "\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as fd:\n",
    "                data = json.load(fd)\n",
    "\n",
    "                title = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'citation-title'], None)\n",
    "                \n",
    "                pub_year = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'year'], None)\n",
    "                pub_month = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'month'], None)\n",
    "                pub_day = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'day'], None)\n",
    "                pub_date = f\"{pub_day}/{pub_month}/{pub_year}\" if pub_year and pub_month and pub_day else None\n",
    "\n",
    "                abstract = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'abstracts'], None)\n",
    "\n",
    "                language = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'citation-info', 'citation-language', '@language'], None)\n",
    "\n",
    "                ref_count = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'tail', 'bibliography', '@refcount'], None)\n",
    "\n",
    "                citedby_count = safe_get(data, ['abstracts-retrieval-response', 'coredata', 'citedby-count'], None)\n",
    "\n",
    "                authors_list = safe_get(data, ['abstracts-retrieval-response', 'authors', 'author'], None)\n",
    "                author_id = [author.get('@auid', None) for author in authors_list if isinstance(author, dict)] if authors_list else None\n",
    "\n",
    "                subject_areas_list = safe_get(data, ['abstracts-retrieval-response', 'subject-areas', 'subject-area'], None)\n",
    "                subject_areas_id = [subject_area.get('@code', None) for subject_area in subject_areas_list if isinstance(subject_area, dict)] if subject_areas_list else None\n",
    "\n",
    "                # Keywords extraction (Fix applied here to match your working example)\n",
    "                authkeywords = safe_get(data, ['abstracts-retrieval-response', 'authkeywords'], None)\n",
    "                keywords = None\n",
    "                if authkeywords and 'author-keyword' in authkeywords:\n",
    "                    keywords = [k['$'] for k in authkeywords['author-keyword'] if isinstance(k, dict)]\n",
    "                keywords = check_slice_or_single(keywords)\n",
    "\n",
    "                # Handling idxterms\n",
    "                idxterms_data = safe_get(data, ['abstracts-retrieval-response', 'idxterms'], None)\n",
    "                if idxterms_data:\n",
    "                    if isinstance(idxterms_data, dict):  # If idxterms is a dictionary\n",
    "                        mainterm = idxterms_data.get('mainterm')\n",
    "                        if isinstance(mainterm, list):\n",
    "                            idxterms = [i.get('$', None) for i in mainterm]  # Extract the '$' value from each item in the list\n",
    "                        elif isinstance(mainterm, dict):\n",
    "                            idxterms = [mainterm.get('$', None)]  # If 'mainterm' is a single dict, extract the '$'\n",
    "                        else:\n",
    "                            idxterms = None\n",
    "                    elif isinstance(idxterms_data, list):  # If idxterms is a list\n",
    "                        idxterms = [i.get('$', None) for i in idxterms_data if isinstance(i, dict)]  # Loop through and extract '$'\n",
    "                    else:\n",
    "                        idxterms = None\n",
    "                else:\n",
    "                    idxterms = None\n",
    "\n",
    "                # Extracting classification codes\n",
    "                classification_list = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'enhancement', 'classificationgroup', 'classifications'], None)\n",
    "                classification_code = get_classification_codes(classification_list)\n",
    "                classification_code = check_slice_or_single(classification_code)\n",
    "\n",
    "                writer.writerow({\n",
    "                    'pid': filename,\n",
    "                    'title': title,\n",
    "                    'pub_date': pub_date,\n",
    "                    'abstract': abstract,\n",
    "                    'language': language,\n",
    "                    'ref_count': ref_count,\n",
    "                    'citedby_count': citedby_count,\n",
    "                    'author_id': author_id,\n",
    "                    'subject_areas_id': subject_areas_id,\n",
    "                    'keywords': keywords,\n",
    "                    'idxterms': idxterms,\n",
    "                    'classification_code': classification_code\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "print(f\"Data extraction complete. Results saved to {output_csv}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
