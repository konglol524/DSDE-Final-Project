{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "import pycountry\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "from geopy.geocoders import Nominatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's pre-trained language model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_FILE_NAME = 'city'\n",
    "COCITY_FILE_NAME = 'cocity'\n",
    "GEOLOCATOR = Nominatim(user_agent=\"isdjuiodfgdfjnjf847hn5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_country(name):\n",
    "    \"\"\"Check if a name is a valid country.\"\"\"\n",
    "    try:\n",
    "        return pycountry.countries.search_fuzzy(name)[0].name\n",
    "    except LookupError:\n",
    "        return False\n",
    "\n",
    "def extract_city_country(text):\n",
    "    \"\"\"Extract one major city and one country using NLP.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    cities = []\n",
    "    country = None\n",
    "    for ent in reversed(doc.ents):  # Iterate in reverse\n",
    "        if ent.label_ == \"GPE\":\n",
    "            res = is_country(ent.text)\n",
    "            if res and not country:\n",
    "                country = res\n",
    "            elif not is_country(ent.text):\n",
    "                if not ent.text.isdigit():\n",
    "                    cities.append(ent.text)\n",
    "    # Return the first city as the \"major city\" (if any) and the first country\n",
    "    city = cities[0] if cities else None\n",
    "    return city, country\n",
    "\n",
    "# Utility functions\n",
    "def find_similar_city(city, city_dict, threshold=90):\n",
    "    \"\"\"Find a similar city in city_dict using fuzzy matching.\"\"\"\n",
    "    if not city_dict:\n",
    "        return None  # No cities to compare\n",
    "    closest_match, similarity = process.extractOne(city, city_dict.keys(), scorer=fuzz.token_sort_ratio)\n",
    "    return closest_match if similarity >= threshold else None\n",
    "\n",
    "\n",
    "def initialize_city_file(city_file):\n",
    "    \"\"\"Initialize city CSV if it does not exist.\"\"\"\n",
    "    city_df = pd.read_csv(city_file)\n",
    "    city_dict = {row[\"city\"]: row[\"city_id\"] for _, row in city_df.iterrows()}\n",
    "    return city_df, city_dict\n",
    "\n",
    "\n",
    "def get_geolocation(city, country):\n",
    "    \"\"\"Fetch latitude and longitude for a city.\"\"\"\n",
    "    geo = GEOLOCATOR.geocode(f\"{city}, {country}\")\n",
    "    if geo:\n",
    "        return geo.latitude, geo.longitude\n",
    "    return None, None\n",
    "\n",
    "def write_cocity_links(cocity_file, city_country_set, city_dict, filename):\n",
    "    \"\"\"Write co-city links to the cocity file.\"\"\"\n",
    "    with open(cocity_file, \"a\", newline=\"\", encoding=\"utf-8\") as cocity_csvfile:\n",
    "        writer = csv.writer(cocity_csvfile)\n",
    "        for city1, country1 in city_country_set:\n",
    "            for city2, country2 in city_country_set:\n",
    "                if city1 != city2:  # Avoid linking same city\n",
    "                    city_id1 = city_dict[city1]\n",
    "                    city_id2 = city_dict[city2]\n",
    "                    writer.writerow([city_id1, city_id2, filename])\n",
    "\n",
    "def update_city_data(city_df, city_dict, city, country, citedby_count):\n",
    "    \"\"\"Update city data and return the modified city_df and city_dict.\"\"\"\n",
    "    if city in city_dict:\n",
    "        city_id = city_dict[city]\n",
    "        city_df.loc[city_df[\"city_id\"] == city_id, \"citation_sum\"] += int(citedby_count)\n",
    "        city_df.loc[city_df[\"city_id\"] == city_id, \"p_count\"] += 1\n",
    "    else:\n",
    "        similar_city = find_similar_city(city, city_dict)\n",
    "        if similar_city:\n",
    "            city_id = city_dict[similar_city]\n",
    "            city_df.loc[city_df[\"city_id\"] == city_id, \"citation_sum\"] += int(citedby_count)\n",
    "            city_df.loc[city_df[\"city_id\"] == city_id, \"p_count\"] += 1\n",
    "        else:\n",
    "            # No similar city found, add as a new entry\n",
    "            city_id = len(city_dict) + 1\n",
    "            city_dict[city] = city_id\n",
    "            lat, lon = get_geolocation(city, country)\n",
    "            # Ensure new_row has the correct structure\n",
    "            new_row = pd.DataFrame(\n",
    "                [{\n",
    "                    \"city_id\": city_id,\n",
    "                    \"city\": city,\n",
    "                    \"country\": country,\n",
    "                    \"citation_sum\": int(citedby_count),\n",
    "                    \"p_count\": 1,\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon,\n",
    "                }],\n",
    "                columns=city_df.columns  # Ensure alignment with city_df structure\n",
    "            )\n",
    "            city_df = pd.concat([city_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return city_df, city_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Moscow', 'Russian Federation')\n"
     ]
    }
   ],
   "source": [
    "print(extract_city_country(\"Moscow Russia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n",
      "set()\n",
      "{('New York City', 'United States')}\n",
      "{('Boston', 'United States')}\n",
      "{('Riyadh', 'Saudi Arabia'), ('Berkeley', 'United States'), ('Leipzig', 'Germany'), ('Chicago', 'United States')}\n",
      "set()\n",
      "{('Moscow', 'Russian Federation'), ('Bozeman', 'United States'), ('St. Petersburg', 'Russian Federation'), ('Piscataway', 'United States')}\n",
      "{('Atlanta', 'United States'), ('Princeton', 'United States'), ('Chicago', 'United States')}\n",
      "set()\n",
      "{('Cape Town', 'South Africa')}\n",
      "set()\n",
      "set()\n",
      "{('Munich', 'Germany'), ('Martinsried', 'Germany')}\n",
      "{('East Lansing', 'United States')}\n",
      "set()\n",
      "set()\n",
      "{('Epalinges', 'Switzerland')}\n",
      "set()\n",
      "set()\n",
      "Saved all articles\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_pages = 1\n",
    "num_papers = 10\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "try:\n",
    "    for page in range(1, num_pages + 1):\n",
    "        current_page = str(page)\n",
    "        url = f'https://www.nature.com/nature/articles?sort=PubDate&year=2024&page={current_page}'\n",
    "        r = requests.get(url, headers={'Accept-Language': 'en-US,en;q=0.5'})\n",
    "\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "        for article in soup.find_all('article'):\n",
    "            rel_link = article.find(\n",
    "                'a', {'data-track-action': 'view article'})['href']\n",
    "            abs_link = 'https://nature.com' + rel_link\n",
    "            r2 = requests.get(abs_link)\n",
    "            soup_2 = BeautifulSoup(r2.content, 'html.parser')\n",
    "            \n",
    "            city_file = f\"../CSVs/{CITY_FILE_NAME}.csv\"\n",
    "            #! Create new city.csv if it doesn't exist\n",
    "            city_df, city_dict = initialize_city_file(city_file)\n",
    "            \n",
    "            extracted_data = []\n",
    "            affi_info = soup_2.select('.c-article-author-affiliation__address')\n",
    "            city_country_set = set()\n",
    "            for affi in affi_info:\n",
    "                city, country = extract_city_country(affi.text)\n",
    "                if city and country:\n",
    "                    city_country_set.add((city, country))\n",
    "            \n",
    "            if(len(city_country_set))\n",
    "            print(city_country_set)\n",
    "            #update city data\n",
    "            #write cocity links\n",
    "                #Get a set of cities and countries\n",
    "                #\n",
    "            \n",
    "\n",
    "        if page >= num_pages:\n",
    "            break\n",
    "\n",
    "    print(\"Saved all articles\")\n",
    "\n",
    "except AttributeError:\n",
    "    print('Invalid entry!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
