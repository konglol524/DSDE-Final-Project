{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from lightgbm import LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Evaluation:\n",
            "Mean Absolute Error: 71.72\n",
            "R^2 Score: -0.06\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_data(file_path):\n",
        "    city_df = pd.read_csv(file_path)\n",
        "    city_df['avg_citation_sum'] = city_df['citation_sum'] / city_df['p_count']\n",
        "    city_df = city_df.dropna(subset=['population', 'gdp_per_capita', 'lat', 'lon', 'safety_index', 'primary_language', 'avg_citation_sum'])\n",
        "\n",
        "    # Log transformation for skewed features\n",
        "    city_df['log_population'] = np.log1p(city_df['population'])\n",
        "    city_df['log_gdp_per_capita'] = np.log1p(city_df['gdp_per_capita'])\n",
        "\n",
        "    # Create GDP total feature\n",
        "    city_df['gdp_total'] = city_df['population'] * city_df['gdp_per_capita']\n",
        "\n",
        "    # Create safety-to-economic ratio\n",
        "    city_df['safety_to_gdp'] = city_df['safety_index'] / city_df['gdp_per_capita']\n",
        "\n",
        "    # Language family mapping\n",
        "    language_family_mapping = {\n",
        "        'English': 'Indo-European', 'Japanese': 'Japonic', 'Spanish': 'Indo-European', \n",
        "        'Thai': 'Tai-Kadai', 'French': 'Indo-European', 'Arabic': 'Afro-Asiatic', \n",
        "        'German': 'Indo-European', 'Chinese': 'Sino-Tibetan', 'Italian': 'Indo-European',\n",
        "        'Portuguese': 'Indo-European', 'Indonesian': 'Austronesian', 'Turkish': 'Turkic',\n",
        "        'Dutch': 'Indo-European', 'Korean': 'Koreanic', 'Vietnamese': 'Austroasiatic', \n",
        "        'Persian (Farsi)': 'Indo-European', 'Polish': 'Indo-European', 'Russian': 'Indo-European', \n",
        "        'Swedish': 'Indo-European', 'Norwegian Nynorsk': 'Indo-European', 'Danish': 'Indo-European',\n",
        "        'Greek': 'Indo-European', 'Afrikaans': 'Indo-European', 'Burmese': 'Sino-Tibetan',\n",
        "        'Guaraní': 'Tupian', 'Romanian': 'Indo-European', 'Ukrainian': 'Indo-European', \n",
        "        'Sinhala': 'Dravidian-Indic', 'Hungarian': 'Uralic', 'Bengali': 'Dravidian-Indic', \n",
        "        'Czech': 'Indo-European', 'Nepali': 'Dravidian-Indic', 'Slovak': 'Indo-European', \n",
        "        'Finnish': 'Uralic', 'Amharic': 'Afro-Asiatic', 'Serbian': 'Indo-European', \n",
        "        'Dzongkha': 'Sino-Tibetan', 'Slovene': 'Indo-European', 'Bulgarian': 'Indo-European', \n",
        "        'Aymara': 'Aymaran', 'Croatian': 'Indo-European', 'Belarusian': 'Indo-European', \n",
        "        'Kazakh': 'Turkic', 'Macedonian': 'Indo-European', 'Lao': 'Austroasiatic', \n",
        "        'Bosnian': 'Indo-European', 'Latvian': 'Indo-European', 'Khmer': 'Austroasiatic', \n",
        "        'Estonian': 'Uralic', 'Lithuanian': 'Indo-European', 'Belizean Creole': 'Creole', \n",
        "        'Armenian': 'Indo-European', 'Albanian': 'Indo-European', 'Icelandic': 'Indo-European',\n",
        "        'Dari': 'Indo-European', 'Mongolian': 'Mongolic', 'Montenegrin': 'Indo-European',\n",
        "        'Chibarwe': 'Niger-Congo', 'Seychellois Creole': 'Creole', 'Malay': 'Austronesian',\n",
        "        'Azerbaijani': 'Turkic', 'Maldivian': 'Indo-Aryan', 'Kyrgyz': 'Turkic'\n",
        "    }\n",
        "    city_df['language_family'] = city_df['primary_language'].map(language_family_mapping).fillna('Other')\n",
        "\n",
        "    return city_df\n",
        "\n",
        "# Model Training and Evaluation Function\n",
        "def evaluate_with_feature_engineering(city_df, feature_columns, target_column):\n",
        "    # Define preprocess pipeline\n",
        "    preprocess = ColumnTransformer(transformers=[\n",
        "        ('num', StandardScaler(), feature_columns),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['language_family'])\n",
        "    ])\n",
        "\n",
        "    # Model pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocess', preprocess),\n",
        "        ('model', RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    # Split data\n",
        "    X = city_df[feature_columns + ['language_family']]\n",
        "    y = city_df[target_column]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return mae, r2, pipeline\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"__main__\":\n",
        "    city_df = preprocess_data('../For_vizml/city_impute_pop_real.csv')\n",
        "\n",
        "    # Define features and target\n",
        "    feature_columns = ['log_population', 'log_gdp_per_capita', 'safety_index', 'gdp_total', 'safety_to_gdp']\n",
        "    target_column = 'avg_citation_sum'\n",
        "    #target_column = 'p_count'\n",
        "\n",
        "    mae, r2, best_model = evaluate_with_feature_engineering(city_df, feature_columns, target_column)\n",
        "\n",
        "    print(\"Model Evaluation:\")\n",
        "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "    print(f\"R^2 Score: {r2:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "0.28\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Evaluation:\n",
            "Best R^2 Score: 0.28\n",
            "Best Mean Absolute Error: 19.72\n",
            "Best Features: ['population', 'gdp_per_capita', 'lon', 'safety_index', 'primary_language']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings for clean output\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_data(file_path):\n",
        "    city_df = pd.read_csv(file_path)\n",
        "    city_df['avg_citation_sum'] = city_df['citation_sum'] / city_df['p_count']\n",
        "    city_df = city_df.dropna(subset=['population', 'gdp_per_capita', 'lat', 'lon', 'safety_index', 'primary_language', 'avg_citation_sum'])\n",
        "    city_df['primary_language'] = city_df['primary_language'].apply(lambda x: 1 if x == 'English' else 0)\n",
        "    \n",
        "    # Remove outliers using IQR\n",
        "    Q1 = city_df['avg_citation_sum'].quantile(0.25)\n",
        "    Q3 = city_df['avg_citation_sum'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    city_df_filtered = city_df[(city_df['avg_citation_sum'] >= lower_bound) & (city_df['avg_citation_sum'] <= upper_bound)]\n",
        "    \n",
        "    return city_df_filtered\n",
        "\n",
        "# Model Training and Evaluation Function with Stratified Sampling\n",
        "def evaluate_features_with_stratified_sampling(city_df, all_features):\n",
        "    best_r2 = float('-inf')\n",
        "    best_mae = float('inf')\n",
        "    best_features = None\n",
        "    best_model = None\n",
        "\n",
        "    # Create bins for stratified sampling\n",
        "    city_df['stratify_bins'] = pd.qcut(city_df['avg_citation_sum'], q=5, labels=False)\n",
        "\n",
        "    for features in [list(comb) for i in range(1, len(all_features) + 1) for comb in combinations(all_features, i)]:\n",
        "        X = city_df[features]\n",
        "        y = city_df['avg_citation_sum']\n",
        "        stratify_bins = city_df['stratify_bins']\n",
        "\n",
        "        # Stratified train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, stratify=stratify_bins, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Use a pipeline with scaling and RandomForestRegressor\n",
        "        pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),  # Scale features\n",
        "            ('model', RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1))\n",
        "        ])\n",
        "\n",
        "        # Train the model\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_mae = mae\n",
        "            best_features = features\n",
        "            best_model = pipeline\n",
        "\n",
        "    return best_r2, best_mae, best_features, best_model\n",
        "\n",
        "# Main Script\n",
        "if __name__ == \"__main__\":\n",
        "    city_df_filtered = preprocess_data('../For_vizml/city_impute_pop_real.csv')\n",
        "    all_features = ['population', 'gdp_per_capita', 'lat', 'lon', 'safety_index', 'primary_language']\n",
        "\n",
        "    best_r2, best_mae, best_features, best_model = evaluate_features_with_stratified_sampling(city_df_filtered, all_features)\n",
        "\n",
        "    print(\"Best Model Evaluation:\")\n",
        "    print(f\"Best R^2 Score: {best_r2:.2f}\")\n",
        "    print(f\"Best Mean Absolute Error: {best_mae:.2f}\")\n",
        "    print(f\"Best Features: {best_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['English', 'Japanese', 'Spanish', 'Thai', 'French', 'Arabic', 'German',\n",
              "       'Chinese', 'Italian', 'Portuguese', 'Indonesian', 'Turkish', 'Dutch',\n",
              "       'Korean', 'Vietnamese', 'Persian (Farsi)', 'Polish', 'Russian',\n",
              "       'Swedish', 'Norwegian Nynorsk', 'Danish', 'Greek', 'Afrikaans',\n",
              "       'Burmese', 'Guaraní', 'Romanian', 'Ukrainian', 'Sinhala', 'Hungarian',\n",
              "       'Bengali', 'Czech', 'Nepali', 'Slovak', 'Finnish', 'Amharic', 'Serbian',\n",
              "       'Dzongkha', 'Slovene', 'Bulgarian', 'Aymara', 'Croatian', 'Belarusian',\n",
              "       'Kazakh', 'Macedonian', 'Lao', 'Bosnian', 'Latvian', 'Khmer',\n",
              "       'Estonian', 'Lithuanian', 'Belizean Creole', 'Armenian', 'Albanian',\n",
              "       'Icelandic', 'Dari', 'Mongolian', 'Montenegrin', 'Chibarwe',\n",
              "       'Seychellois Creole', 'Malay', 'Azerbaijani', 'Maldivian', 'Kyrgyz'],\n",
              "      dtype='object', name='primary_language')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "city_df = pd.read_csv('../For_vizml/city_impute_pop_real.csv')\n",
        "city_df['primary_language'].value_counts().index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Average Citation Sum for the selected city: 39.35\n",
            "Random City Details:\n",
            "      city_id      city country  citation_sum  p_count        lat         lon  \\\n",
            "1852     1853  Kanazawa   Japan          1264       18  36.561627  136.656882   \n",
            "\n",
            "      population  gdp_per_capita   time_zone  safety_index  primary_language  \\\n",
            "1852    466029.0    33834.392106  Asia/Tokyo          77.1                 0   \n",
            "\n",
            "      avg_citation_sum  stratify_bins  \n",
            "1852         70.222222              4  \n"
          ]
        }
      ],
      "source": [
        "# Predict for a random city\n",
        "random_city = city_df_filtered.sample(n=1, random_state=10)\n",
        "city_features = random_city[best_features]\n",
        "predicted_avg_citation_sum = best_model.predict(city_features)[0]\n",
        "\n",
        "print(f\"Predicted Average Citation Sum for the selected city: {predicted_avg_citation_sum:.2f}\")\n",
        "print(\"Random City Details:\")\n",
        "print(random_city)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Set (new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows with NaN dropped, and train and test datasets are ready!\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "city_df = pd.read_csv('../For_vizml/city_impute_pop_real.csv')\n",
        "\n",
        "# Create a new column avg_citation_sum by dividing citation_sum by p_count\n",
        "city_df['avg_citation_sum'] = city_df['citation_sum'] / city_df['p_count']\n",
        "\n",
        "# Drop rows with missing values in relevant columns\n",
        "city_df = city_df.dropna(subset=['population', 'gdp_per_capita', 'lat', 'lon', 'safety_index', 'primary_language', 'avg_citation_sum'])\n",
        "\n",
        "# Log transform skewed variables\n",
        "city_df['log_population'] = np.log1p(city_df['population'])\n",
        "city_df['log_gdp_per_capita'] = np.log1p(city_df['gdp_per_capita'])\n",
        "\n",
        "# One-hot encode primary_language\n",
        "city_df = pd.get_dummies(city_df, columns=['primary_language'], drop_first=True)\n",
        "\n",
        "# KMeans clustering for geospatial grouping\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "city_df['geo_cluster'] = kmeans.fit_predict(city_df[['lat', 'lon']])\n",
        "\n",
        "# Select the features and target variable\n",
        "X = city_df[['log_population', 'log_gdp_per_capita', 'lat', 'lon', 'safety_index', 'geo_cluster']]\n",
        "y = np.log1p(city_df['avg_citation_sum'])  # Log-transform the target\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=city_df['geo_cluster']\n",
        ")\n",
        "\n",
        "print(\"Rows with NaN dropped, and train and test datasets are ready!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "testenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
