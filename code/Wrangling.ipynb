{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_get(data, path, default=None):\n",
    "    for key in path:\n",
    "        if isinstance(data, dict):\n",
    "            data = data.get(key, default)\n",
    "        elif isinstance(data, list) and isinstance(key, int) and 0 <= key < len(data):\n",
    "            data = data[key]\n",
    "        else:\n",
    "            return default\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Authors' info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract author info\n",
    "data_dir = 'Project_data'\n",
    "output_file = 'authors.csv'\n",
    "\n",
    "# Use a set to track unique author IDs\n",
    "unique_authors = set()\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header once\n",
    "    writer.writerow(['at_id', 'name', 'degree'])\n",
    "\n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if filename.startswith('20'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    print(file_path)\n",
    "                    try:\n",
    "                        data = json.load(file)\n",
    "                        # Access authors\n",
    "                        authors = data.get('abstracts-retrieval-response', {}).get('authors', {}).get('author', [])\n",
    "                        for author in authors:\n",
    "                            at_id = author.get(\"@auid\")\n",
    "                            if at_id and at_id not in unique_authors:  # Avoid duplicates\n",
    "                                name = f\"{author.get('ce:given-name', '')} {author.get('ce:surname', '')}\".strip()\n",
    "                                degree = author.get('ce:degrees', 'NA')\n",
    "                                writer.writerow([at_id, name, degree])\n",
    "                                unique_authors.add(at_id)  # Add the ID to the set\n",
    "                    except (json.JSONDecodeError, KeyError) as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Affiliations Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract affiliation info\n",
    "data_dir = 'Sample_data'\n",
    "output_file = 'affiliations.csv'\n",
    "unique_affiliations = set()\n",
    "\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['af_id', 'name', 'organization', 'country', 'city'])\n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if filename.startswith('20'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    \n",
    "                    try:\n",
    "                        data = json.load(file)\n",
    "                        # Access authors\n",
    "                        auth_groups = data.get('abstracts-retrieval-response', {}).get('item', {}).get('bibrecord', {}).get('head', {}).get('author-group', [])\n",
    "\n",
    "                        if isinstance(auth_groups, list):\n",
    "                            for auth_group in auth_groups:\n",
    "                                affiliation = auth_group.get('affiliation', {})   \n",
    "                                af_id = affiliation.get(\"@afid\")\n",
    "                                if af_id and af_id not in unique_affiliations:  # Avoid duplicates\n",
    "                                    org_list = affiliation.get('organization', 'NA')\n",
    "                                    if isinstance(org_list, list):\n",
    "                                        organization = ', '.join([org['$'] for org in org_list])\n",
    "                                        name = org_list[-1]['$']\n",
    "                                    elif isinstance(org_list, dict):\n",
    "                                        organization = org_list.get('$', 'NA')\n",
    "                                        name = organization\n",
    "                                    else:\n",
    "                                        organization = affiliation.get('ce:text', 'NA')\n",
    "                                        name = organization\n",
    "                                    country = affiliation.get('country', 'NA')\n",
    "                                    city = affiliation.get('city', 'NA')\n",
    "                                    writer.writerow([af_id, name, organization, country, city])\n",
    "                                    unique_affiliations.add(af_id)  # Add the ID to the set\n",
    "                        else:\n",
    "                            affiliation = auth_groups.get('affiliation', {}) \n",
    "                            af_id = affiliation.get(\"@afid\")\n",
    "                            if af_id and af_id not in unique_affiliations:  # Avoid duplicates\n",
    "                                org_list = affiliation.get('organization', 'NA')\n",
    "                                if isinstance(org_list, list):\n",
    "                                    organization = ', '.join([org['$'] for org in org_list])\n",
    "                                    name = org_list[-1]['$']\n",
    "                                elif isinstance(org_list, dict):\n",
    "                                    organization = org_list.get('$', 'NA')\n",
    "                                    name = organization\n",
    "                                else:\n",
    "                                    organization = affiliation.get('ce:text', 'NA')\n",
    "                                    name = organization\n",
    "                            country = affiliation.get('country', 'NA')\n",
    "                            city = affiliation.get('city', 'NA')\n",
    "                            writer.writerow([af_id, name, organization, country, city])\n",
    "                            unique_affiliations.add(af_id)  # Add the ID to the set\n",
    "                    except (json.JSONDecodeError, KeyError) as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author to Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Project_data'\n",
    "output_file = 'author_to_affi.csv'\n",
    "columns = ['pid', 'at_id', 'af_id']\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if filename.startswith('20'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    \n",
    "                    try:\n",
    "                        data = json.load(file)\n",
    "                        auth_groups = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'author-group'], None)\n",
    "                        if isinstance(auth_groups, list):\n",
    "                            for auth_group in auth_groups:\n",
    "                                affiliation = auth_group.get('affiliation', {})   \n",
    "                                af_id = affiliation.get(\"@afid\")\n",
    "                                authors = auth_group.get('author', [])\n",
    "                                for author in authors:\n",
    "                                    at_id = author.get('@auid')\n",
    "                                    writer.writerow({'pid': filename, 'at_id': at_id, 'af_id': af_id})\n",
    "                        else:\n",
    "                            affiliation = auth_groups.get('affiliation', {})\n",
    "                            af_id = affiliation.get(\"@afid\")\n",
    "                            authors = auth_groups.get('author', [])\n",
    "                            for author in authors:\n",
    "                                at_id = author.get('@auid')\n",
    "                                writer.writerow({'pid': filename, 'at_id': at_id, 'af_id': af_id})\n",
    "                    except Exception as e:\n",
    "                            print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract subject area data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Project_data'\n",
    "output_file = 'subject_areas.csv'\n",
    "\n",
    "columns = [\n",
    "    'subject_area_id', 'subject_area_name',\n",
    "]\n",
    "\n",
    "unique_subject = set()\n",
    "\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if filename.startswith('20'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    try:\n",
    "                        data = json.load(file)\n",
    "                        subject_areas_list = safe_get(data, ['abstracts-retrieval-response', 'subject-areas', 'subject-area'], None)\n",
    "                        for subject in subject_areas_list:\n",
    "                            subject_id = subject.get('@code', None)\n",
    "                            subject_name = subject.get('$', None)\n",
    "                            if subject_id not in unique_subject:\n",
    "                                writer.writerow({\n",
    "                                    'subject_area_id': subject_id,\n",
    "                                    'subject_area_name': subject_name\n",
    "                                })\n",
    "                                unique_subject.add(subject_id)\n",
    "                    except Exception as e:\n",
    "                            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Paper Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_codes(classification_list):\n",
    "    \"\"\"\n",
    "    Processes the classification list and extracts the relevant classification code.\n",
    "    \n",
    "    :param classification_list: List of classifications, can be of different types.\n",
    "    :return: List of classification codes.\n",
    "    \"\"\"\n",
    "    classification_codes = []\n",
    "    \n",
    "    for classification in classification_list:\n",
    "        if isinstance(classification, dict):\n",
    "            classification_type = classification.get('@type')\n",
    "            classification_data = classification.get('classification')\n",
    "            \n",
    "            if classification_type == 'SUBJABBR' or classification_type == 'ASJC':\n",
    "                # For SUBJABBR and AJSC, classification might be a single string or a list\n",
    "                if isinstance(classification_data, list):\n",
    "                    classification_codes.extend([item.get('$') for item in classification_data if isinstance(item, dict)])\n",
    "                else:\n",
    "                    classification_codes.append(classification_data)  # Single string classification\n",
    "            \n",
    "            elif classification_type in ['CPXCLASS', 'FLXCLASS']:\n",
    "                if isinstance(classification_data, list):\n",
    "                    classification_codes.extend([item.get('classification-code') for item in classification_data if isinstance(item, dict)])\n",
    "                elif isinstance(classification_data, dict):\n",
    "                    classification_codes.append(classification_data.get('classification-code'))\n",
    "    \n",
    "    return classification_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_slice_or_single(data):\n",
    "    #Ensures that the returned data is always a list, even if it's a single item.\n",
    "    if data is None:\n",
    "        return None\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    return [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleidx(idxterms_data):\n",
    "    if idxterms_data:\n",
    "        if isinstance(idxterms_data, dict):  # If idxterms is a dictionary\n",
    "            mainterm = idxterms_data.get('mainterm')\n",
    "            if isinstance(mainterm, list):\n",
    "                idxterms = [i.get('$', None) for i in mainterm]  # Extract the '$' value from each item in the list\n",
    "            elif isinstance(mainterm, dict):\n",
    "                idxterms = [mainterm.get('$', None)]  # If 'mainterm' is a single dict, extract the '$'\n",
    "            else:\n",
    "                idxterms = None\n",
    "        elif isinstance(idxterms_data, list):  # If idxterms is a list\n",
    "            idxterms = [i.get('$', None) for i in idxterms_data if isinstance(i, dict)]  # Loop through and extract '$'\n",
    "        else:\n",
    "            idxterms = None\n",
    "    else:\n",
    "        idxterms = None\n",
    "    return idxterms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Sample_data'\n",
    "output_file = 'papers.csv'\n",
    "\n",
    "columns = [\n",
    "    'pid','title', 'pub_date', 'abstract', 'language', 'ref_count',\n",
    "    'citedby_count', 'corresponding_author', 'author_id', 'subject_areas_id', 'keywords',\n",
    "    'idxterms', 'classification_code'\n",
    "]\n",
    "cnt = 0\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    writer.writeheader()\n",
    "    # Walk through all subdirectories and files\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if cnt == 40: break\n",
    "            cnt+=1\n",
    "            if filename.startswith('20'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    try:\n",
    "                        data = json.load(file)\n",
    "                        title = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'citation-title'], None)\n",
    "                        pub_year = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'year'], None)\n",
    "                        pub_month = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'month'], None)\n",
    "                        pub_day = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'source', 'publicationdate', 'day'], None)\n",
    "                        pub_date = f\"{pub_day}/{pub_month}/{pub_year}\" if pub_year and pub_month and pub_day else None\n",
    "                        abstract = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'abstracts'], None)\n",
    "                        language = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'citation-info', 'citation-language', '@language'], None)\n",
    "                        ref_count = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'tail', 'bibliography', '@refcount'], None)\n",
    "                        citedby_count = safe_get(data, ['abstracts-retrieval-response', 'coredata', 'citedby-count'], None)\n",
    "                        \n",
    "                        \n",
    "                        authors_list = safe_get(data, ['abstracts-retrieval-response', 'authors', 'author'], None)\n",
    "                        author_dict= {f\"{author.get('ce:given-name', '')} {author.get('ce:surname', '')}\".strip() : author.get('@auid', None) for author in authors_list if isinstance(author, dict)} if authors_list else None\n",
    "                        author_id = list(author_dict.values())\n",
    "                        cor_author = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'correspondence', 'person'], None)\n",
    "                        ca_name = f\"{safe_get(cor_author, ['ce:given-name'], '')} {safe_get(cor_author, ['ce:surname'], '')}\".strip()\n",
    "                        \n",
    "                        coressponding_auid = author_dict.get(ca_name, None)\n",
    "\n",
    "                        subject_areas_list = safe_get(data, ['abstracts-retrieval-response', 'subject-areas', 'subject-area'], None)\n",
    "                        subject_areas_id = [subject_area.get('@code', None) for subject_area in subject_areas_list if isinstance(subject_area, dict)] if subject_areas_list else None\n",
    "                        \n",
    "                        authkeywords = safe_get(data, ['abstracts-retrieval-response', 'authkeywords'], None)\n",
    "                        keywords = None\n",
    "                        if authkeywords and 'author-keyword' in authkeywords:\n",
    "                            authkeyword = authkeywords.get('author-keyword', None)\n",
    "                            if(isinstance(authkeyword, dict)):\n",
    "                                keywords = re.sub(r'(?<=[\\s])([A-Z])', r' \\1', authkeyword.get('$', None)).split('  ')\n",
    "                            else:\n",
    "                                keywords = [k['$'] for k in authkeyword if isinstance(k, dict)]                            \n",
    "                        keywords = check_slice_or_single(keywords)\n",
    "                        \n",
    "                        idxterms_data = safe_get(data, ['abstracts-retrieval-response', 'idxterms'], None)\n",
    "                        idxterms = handleidx(idxterms_data)\n",
    "                        \n",
    "                        classification_list = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'enhancement', 'classificationgroup', 'classifications'], None)\n",
    "                        classification_code = get_classification_codes(classification_list)\n",
    "                        classification_code = check_slice_or_single(classification_code)\n",
    "                        \n",
    "                        writer.writerow({\n",
    "                            'pid': filename,\n",
    "                            'corresponding_author': coressponding_auid,\n",
    "                            'title': title,\n",
    "                            'pub_date': pub_date,\n",
    "                            'abstract': abstract,\n",
    "                            'language': language,\n",
    "                            'ref_count': ref_count,\n",
    "                            'citedby_count': citedby_count,\n",
    "                            'author_id': author_id,\n",
    "                            'subject_areas_id': subject_areas_id,\n",
    "                            'keywords': keywords,\n",
    "                            'idxterms': idxterms,\n",
    "                            'classification_code': classification_code\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Phuree---#\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from pprint import pprint\n",
    "geolocator = Nominatim(user_agent=\"isdjuiodfgdfjnjf847hn5\")\n",
    "\n",
    "data_dir = './Sample_data'\n",
    "output_file = '../CSVs/papers-mock.csv'\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for root, _, files in os.walk(data_dir):\n",
    "    for filename in files:\n",
    "        if cnt == 50: break\n",
    "        cnt+=1\n",
    "        if filename.startswith('20'):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                try:\n",
    "                    data = json.load(file)\n",
    "                    \n",
    "                    # Extract organization (institution) information\n",
    "                    org_list = safe_get(data, ['abstracts-retrieval-response', 'item', 'bibrecord', 'head', 'correspondence','affiliation','organization'], None)\n",
    "                    organization = None\n",
    "                    if isinstance(org_list, list):\n",
    "                        organization = org_list[-1]['$'] if org_list else None\n",
    "                    elif isinstance(org_list, dict):\n",
    "                        organization = org_list.get('$', None)\n",
    "\n",
    "                    # city and cocity\n",
    "                    city_file = '../CSVs/city1.csv'\n",
    "                    #! Create new city.csv if it doesn't exist\n",
    "                    if not os.path.exists(city_file):\n",
    "                        city_df = pd.DataFrame(columns=['city_id', 'city', 'country', 'citation_sum', 'p_count','lat','lon'])\n",
    "                        city_dict = {}\n",
    "                        # Add a mock row\n",
    "                        mock_row = {\n",
    "                            'city_id': 1,\n",
    "                            'city': 'Mock City eiei',\n",
    "                            'country': 'Mock Country eiei',\n",
    "                            'citation_sum': 0,\n",
    "                            'p_count': 0,\n",
    "                            'lat': 0,\n",
    "                            'lon': 0\n",
    "                        }\n",
    "                        city_df.loc[-1] = mock_row\n",
    "                    else:\n",
    "                        city_df = pd.read_csv(city_file)\n",
    "\n",
    "                    city_dict = {}\n",
    "                    if city_df.shape[0] != 0:\n",
    "                        city_dict = {row['city']: row['city_id'] for _, row in city_df.iterrows()}\n",
    "\n",
    "                    citedby_count = safe_get(data, ['abstracts-retrieval-response','coredata','citedby-count'], None)\n",
    "                    citedby_count = int(citedby_count) if citedby_count else 0\n",
    "                    # print(\"count\",citedby_count)\n",
    "                    # set (city, country)\n",
    "                    city_country_set = set()\n",
    "                    \n",
    "                    affiliations = safe_get(data, ['abstracts-retrieval-response', 'affiliation'], None)\n",
    "                    # always return list or None\n",
    "                    affiliation_list = check_slice_or_single(affiliations) \n",
    "            \n",
    "                    for affiliation in affiliation_list:\n",
    "                        city = affiliation.get('affiliation-city', None)\n",
    "                        country = affiliation.get('affiliation-country', None)\n",
    "                        # print(city, country)\n",
    "                        if city and country:\n",
    "                            city_country_set.add((city, country))\n",
    "\n",
    "                    # Handle I. Single City, II. Multiple Cities\n",
    "                    if len(city_country_set) == 1:\n",
    "                        #! Case I: Single City\n",
    "                        city, country = list(city_country_set)[0]\n",
    "                        \n",
    "                        if city in city_dict:\n",
    "                            city_id = city_dict[city] \n",
    "                            city_df.loc[city_df['city_id'] == city_id, 'citation_sum'] += int(citedby_count)\n",
    "                            city_df.loc[city_df['city_id'] == city_id, 'p_count'] += 1\n",
    "                        else:\n",
    "                            city_id = len(city_dict) + 1\n",
    "                            city_dict[city] = city_id\n",
    "                            #geolocation \n",
    "                            # geo = geolocator.geocode(f\"{city}, {country}\")\n",
    "                            lat, lon = None, None\n",
    "                            # if geo:\n",
    "                            #     lat, lon = geo.latitude, geo.longitude\n",
    "                                \n",
    "                            new_row = pd.DataFrame([{\n",
    "                                'city_id': city_id,\n",
    "                                'city': city,\n",
    "                                'country': country,\n",
    "                                'citation_sum': int(citedby_count),\n",
    "                                'p_count': 1,\n",
    "                                'lat': lat,\n",
    "                                'lon': lon\n",
    "                            }])\n",
    "                            city_df = pd.concat([city_df, new_row], ignore_index=True)\n",
    "                    else:\n",
    "                        #! Case II: Multiple Cities\n",
    "                        for city, country in city_country_set:\n",
    "                            if city in city_dict:\n",
    "                                city_id = city_dict[city]\n",
    "                                city_df.loc[city_df['city_id'] == city_id, 'citation_sum'] += int(citedby_count)\n",
    "                                city_df.loc[city_df['city_id'] == city_id, 'p_count'] += 1\n",
    "                            else:\n",
    "                                city_id = len(city_dict) + 1\n",
    "                                city_dict[city] = city_id\n",
    "                                #geolocation \n",
    "                                # geo = geolocator.geocode(f\"{city}, {country}\")\n",
    "                                lat, lon = None, None\n",
    "                                # if geo:\n",
    "                                #     lat, lon = geo.latitude, geo.longitude\n",
    "                                new_row = pd.DataFrame([{\n",
    "                                    'city_id': city_id,\n",
    "                                    'city': city,\n",
    "                                    'country': country,\n",
    "                                    'citation_sum': int(citedby_count),\n",
    "                                    'p_count': 1,\n",
    "                                    'lat': lat,\n",
    "                                    'lon': lon\n",
    "                                }])\n",
    "                                city_df = pd.concat([city_df, new_row], ignore_index=True)\n",
    "\n",
    "                        # create cocity.csv and link the cities\n",
    "                        cocity_file = '../CSVs/cocity.csv'\n",
    "                        with open(cocity_file, 'a', newline='', encoding='utf-8') as cocity_csvfile:\n",
    "                            cocity_writer = csv.writer(cocity_csvfile)\n",
    "                            # Write header if file is empty\n",
    "                            if os.stat(cocity_file).st_size == 0:\n",
    "                                cocity_writer.writerow(['city_id1', 'city_id2', 'filename'])\n",
    "                            for city1, country1 in city_country_set:\n",
    "                                for city2, country2 in city_country_set:\n",
    "                                    if city1 != city2: # Avoid linking same city\n",
    "                                        city_id1 = city_dict[city1]\n",
    "                                        city_id2 = city_dict[city2]\n",
    "                                        cocity_writer.writerow([city_id1, city_id2, filename])\n",
    "\n",
    "\n",
    "                    city_df.to_csv(city_file, index=False)\n",
    "                     \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'place_id': 238331378, 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright', 'osm_type': 'relation', 'osm_id': 90337, 'lat': '13.743107349999999', 'lon': '100.53288370292987', 'class': 'amenity', 'type': 'university', 'place_rank': 30, 'importance': 0.5126528276219269, 'addresstype': 'amenity', 'name': 'จุฬาลงกรณ์มหาวิทยาลัย', 'display_name': 'จุฬาลงกรณ์มหาวิทยาลัย, 254, ถนนพญาไท, สามย่าน, แขวงวังใหม่, เขตปทุมวัน, กรุงเทพมหานคร, 10330, ประเทศไทย', 'boundingbox': ['13.7327444', '13.7444053', '100.5244538', '100.5383395']}\n"
     ]
    }
   ],
   "source": [
    "geolocator = Nominatim(user_agent=\"geoapiExedadawrcises111\")\n",
    "\n",
    "# Example usage:\n",
    "city = \"Bangkok\"\n",
    "country = \"Thailand\"\n",
    "x = geolocator.geocode(\"Chulalongkorn University\").raw\n",
    "if x:\n",
    "    print(x)\n",
    "\n",
    "# from geopy.geocoders import Nominatim\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Instantiate a new Nominatim client\n",
    "# app = Nominatim(user_agent=\"tutorial\")\n",
    "\n",
    "# # Get location raw data from the user\n",
    "# your_loc = input(\"Enter your location: \")\n",
    "# location = app.geocode(your_loc).raw\n",
    "\n",
    "# # Print raw data\n",
    "# pprint(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
